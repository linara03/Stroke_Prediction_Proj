{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "df = pd.read_csv(r\"D:\\Stroke_prediction_project\\preprocessed_stroke_data.csv\")\n",
    "\n",
    "# Check the columns - they should already be encoded based on your notebook\n",
    "print(\"Dataset columns:\", df.columns.tolist())\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Verify data types - all should be numeric if preprocessing was done correctly\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Check for any non-numeric columns that need encoding\n",
    "non_numeric_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "if non_numeric_cols:\n",
    "    print(f\"\\nWarning: Found non-numeric columns: {non_numeric_cols}\")\n",
    "    print(\"These need to be encoded before training!\")\n",
    "    # If there are still string columns, encode them\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    for col in non_numeric_cols:\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('Stroke', axis=1)\n",
    "y = df['Stroke']\n",
    "\n",
    "print(f\"\\nFeatures shape: {X.shape}\")\n",
    "print(f\"Target distribution:\\n{y.value_counts()}\")\n",
    "\n",
    "# Get feature names for later use\n",
    "feature_names = X.columns.tolist()\n",
    "print(f\"\\nFeature names ({len(feature_names)}): {feature_names}\")\n",
    "\n",
    "# Apply SMOTE to balance the dataset\n",
    "print(\"\\n--- Applying SMOTE ---\")\n",
    "smote = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "print(f\"After SMOTE:\\n{pd.Series(y_resampled).value_counts()}\")\n",
    "\n",
    "# Train-test split on the resampled data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_resampled, y_resampled,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_resampled\n",
    ")\n",
    "\n",
    "# Optional: Scale the features (RandomForest doesn't require it, but can help)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train Random Forest Classifier\n",
    "print(\"\\n--- Training Random Forest ---\")\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,  # Increase from default 100 for better performance\n",
    "    max_depth=10,      # Prevent overfitting\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    class_weight='balanced'  # Additional balancing\n",
    ")\n",
    "\n",
    "# Train on scaled data\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\n--- Model Evaluation ---\")\n",
    "y_pred_train = model.predict(X_train_scaled)\n",
    "y_pred_test = model.predict(X_test_scaled)\n",
    "\n",
    "print(f\"Training Accuracy: {model.score(X_train_scaled, y_train):.4f}\")\n",
    "print(f\"Test Accuracy: {model.score(X_test_scaled, y_test):.4f}\")\n",
    "\n",
    "print(\"\\n--- Test Set Classification Report ---\")\n",
    "print(classification_report(y_test, y_pred_test, target_names=['No Stroke', 'Stroke']))\n",
    "\n",
    "print(\"\\n--- Confusion Matrix (Test Set) ---\")\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "print(\"True Negatives:\", cm[0,0])\n",
    "print(\"False Positives:\", cm[0,1])\n",
    "print(\"False Negatives:\", cm[1,0])\n",
    "print(\"True Positives:\", cm[1,1])\n",
    "\n",
    "# Feature importance\n",
    "print(\"\\n--- Top 10 Most Important Features ---\")\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "# Test with a sample high-risk patient\n",
    "print(\"\\n--- Testing with High-Risk Patient ---\")\n",
    "# This should be a high-risk profile based on your dataset\n",
    "high_risk_patient = pd.DataFrame([[\n",
    "    70,    # Age (older)\n",
    "    1,     # Sex (1=Male)\n",
    "    1,     # Hypertension (Yes)\n",
    "    1,     # Heart_Disease (Yes)\n",
    "    1,     # Work_Type (encoded value)\n",
    "    1,     # Residence_Type (encoded)\n",
    "    250.0, # Average_Glucose_Level (high)\n",
    "    35.0,  # BMI (high)\n",
    "    2,     # Smoking_Status (Currently=2)\n",
    "    2.0,   # Physical_Activity (low)\n",
    "    10,    # Alcohol_Intake (high)\n",
    "    9,     # Stress_Level (high)\n",
    "    180,   # Blood_Pressure (high)\n",
    "    280,   # Cholesterol (high)\n",
    "    1,     # Family_History (Yes)\n",
    "    85.0   # MRI_Result (if this column exists, high value)\n",
    "]], columns=feature_names)\n",
    "\n",
    "# Scale the test patient\n",
    "high_risk_scaled = scaler.transform(high_risk_patient)\n",
    "\n",
    "# Predict\n",
    "prediction = model.predict(high_risk_scaled)[0]\n",
    "probabilities = model.predict_proba(high_risk_scaled)[0]\n",
    "\n",
    "print(f\"Prediction: {'STROKE RISK' if prediction == 1 else 'NO STROKE RISK'}\")\n",
    "print(f\"Probability of No Stroke: {probabilities[0]:.2%}\")\n",
    "print(f\"Probability of Stroke: {probabilities[1]:.2%}\")\n",
    "\n",
    "# Save the model with scaler and feature names\n",
    "print(\"\\n--- Saving Model ---\")\n",
    "model_package = {\n",
    "    'model': model,\n",
    "    'scaler': scaler,\n",
    "    'feature_names': feature_names,\n",
    "    'version': '2.0',\n",
    "    'training_accuracy': model.score(X_train_scaled, y_train),\n",
    "    'test_accuracy': model.score(X_test_scaled, y_test)\n",
    "}\n",
    "\n",
    "\n",
    "with open(r\"D:\\Stroke_prediction_project\\models\\random_forest_fixed.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_package, f)\n",
    "\n",
    "print(\"Model saved successfully!\")\n",
    "print(f\"Expected {len(feature_names)} features in order: {feature_names}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
